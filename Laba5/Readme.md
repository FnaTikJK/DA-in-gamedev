# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #4 выполнил(а):
- Яськов Антоний Михайлович
- РИ210942
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Ознакомиться с основными операторами зыка Python на примере реализации линейной регрессии.


## Задание 1
### Интегрировать экономическую систему в проект Unity и обучить Ml Agent.
Скачал проект, установил необходимые библиотеки запустил обучение ml-агента
![image](https://user-images.githubusercontent.com/70794890/205243155-41a4daf4-b597-43fd-9ab6-75b7b3c54c52.png)


Установил tensorflow, запустил tensorboard. Поулчил следующие графики

![2022-12-02_12-52-56](https://user-images.githubusercontent.com/70794890/205243278-d293ff84-8ac0-44bf-ac95-d682893876aa.png)

![2022-12-02_12-53-12](https://user-images.githubusercontent.com/70794890/205243285-20f0c581-0622-4607-bb92-753c4340beec.png)




## Задание 2
### Измените параметры файла. yaml-агента и определить какие параметры и как влияют на обучение модели

### num_epoch 
3 => 10
Кол-во эпох обучения(проходов через буфер опыта во время градиентного спуска). Чем меньше значение - тем стабильнее график, за счёт замедления обучения

Как видно график стал состоять всего из 3 линий, что говорит о его резкости
![image](https://user-images.githubusercontent.com/70794890/205248609-ee571774-422d-441d-a9f6-8fa851c712b8.png)

![image](https://user-images.githubusercontent.com/70794890/205248645-480ff42f-ae70-4a4d-8396-67aa24f53e4b.png)


### learning_rate 
3.0e-4 => 0.5e-4

Определяет скорость обучения(длину "шага). В каждой сети имеет своё значение, которое подбирается путём проб и ошибок, тк слишком маленькое значение может оставить ошибку сидеть в яме, а болбое перешагивать лучшие решения и также держать в яме.

Как видно по графикам, после уменьшения модель стала обучаться дольше, но она, в теории, может дать более лучший результат в конце.
![image](https://user-images.githubusercontent.com/70794890/205250444-49db8620-ef95-4944-8242-75c847478404.png)

![image](https://user-images.githubusercontent.com/70794890/205250480-4cf08f28-beb5-4e1d-a7f3-07c3f80fa76a.png)

beta = 1.0e-2 => 1.0e-4

![image](https://user-images.githubusercontent.com/70794890/205252144-561d86d7-c79f-4d6a-ae36-764f97dc1f67.png)

![image](https://user-images.githubusercontent.com/70794890/205252178-17db0a38-b138-41e8-97a2-715fce358e8e.png)

lambd = 0.95 => 0.7

![image](https://user-images.githubusercontent.com/70794890/205253388-d0651812-2d99-45c8-8a2f-9c211f5fb503.png)


![image](https://user-images.githubusercontent.com/70794890/205253539-8fdc66e5-81aa-4055-bc3c-eb4eac943b72.png)


epxilon = 0.2 => 0.3

![image](https://user-images.githubusercontent.com/70794890/205254573-8066350d-04b4-4e64-be56-dd6e9eaeabf7.png)


![image](https://user-images.githubusercontent.com/70794890/205254640-2972a1c8-7fdb-4ac3-83ec-e4a249bd2aff.png)




## Задание 3
### Опишите результаты, выведенные в TensorBoard.

Cumulative reward - cреднее вознаграждение за эпизод. Имеет небольшие изменения. В лучшем случае, должно последовательно увеличиваться с течением времени.

Episode Length - средняя продолжительность эпизода в среде для агентов.

Policy Loss - этот график определяет величину изменения политики со временем. Должен стремиться вниз, показывая, что политика всё лучше принимает решения.

Value Loss - это средняя потеря функции значения. Будет увеличиваться по мере увеличения вознаграждения, а после становления стабильного награждения должно уменьшаться.

Beta - график, показывающий изменение силы энтропийной регуляризации, которая делает политику агента «более случайной».

Entropy - соответствует тому, насколько случайны решения, показывает величину исследования агента. Должно последовательно уменьшаться с течением обучения.

Learning Rate - соответствует скорости обучения, будет постепенно уменьшаться с течением времени.

Extrinsic Reward - соответствует среднему совокупному вознаграждению, полученному от окружающей среды за эпизод.

Value Estimate - это среднее значение, посещённое всеми состояниями агента. Чтобы отражать увеличение знаний агента, это значение должно расти, а затем стабилизироваться.


## Выводы

В результате данной лабораторной работы я познакомился с Перцептроном, научился интегрировать его в Unity, узнал про эпохи обучения, ошибку и их взаимосвязь.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
